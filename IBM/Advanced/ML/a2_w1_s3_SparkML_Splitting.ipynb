{"nbformat_minor": 1, "cells": [{"source": "!rm -Rf HMP_Dataset\n!git clone https://github.com/wchill/HMP_Dataset", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from pyspark.sql.types import StructType, StructField, IntegerType\n\nschema = StructType([\n    StructField(\"x\", IntegerType(), True),\n    StructField(\"y\", IntegerType(), True),\n    StructField(\"z\", IntegerType(), True)])\n\n#df = spark.read.csv(\"user_click_seq.csv\",header=False,schema=schema)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "import os\n\n#get list of folders/files in folder HMP_Dataset\nfile_list = os.listdir('HMP_Dataset')\n\n#filter list for folders containing data\nfile_list_filtered = [s for s in file_list if '_' in s]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "import os\n\n#get list of folders/files in folder HMP_Dataset\nfile_list = os.listdir('HMP_Dataset')\n\n#filter list for folders containing data\nfile_list_filtered = [s for s in file_list if '_' in s]\n\nfrom pyspark.sql.functions import lit\n\n#create pandas data frame for all the data\n\ndf = None\n\nfor category in file_list_filtered:\n    data_files = os.listdir('HMP_Dataset/'+category)\n    \n    #create a temporary pandas data frame for each data file\n    for data_file in data_files:\n        print(data_file)\n        temp_df = spark.read.option(\"header\", \"true\").option(\"header\", \"false\").option(\"delimiter\", \" \").csv('HMP_Dataset/'+category+'/'+data_file,schema=schema)\n        \n        #create a column called \"source\" storing the current CSV file\n        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n        \n        #create a column called \"class\" storing the current data folder\n        temp_df = temp_df.withColumn(\"class\", lit(category))\n        \n        #append to existing data frame list\n        #data_frames = data_frames + [temp_df]\n                                                                                                             \n        if df is None:\n            df = temp_df\n        else:\n            df = df.union(temp_df)\n        \n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df.createOrReplaceTempView('df')\ndf_energy = spark.sql(\"\"\"\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label, class from df group by class\n\"\"\")      \ndf_energy.createOrReplaceTempView('df_energy') ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df_join = spark.sql('select * from df inner join df_energy on df.class=df_energy.class')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "splits = df_join.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df_train.count()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df_test.count()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\n\nmodel = pipeline.fit(df_train)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model.stages[2].summary.r2", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model = pipeline.fit(df_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model.stages[2].summary.r2", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.1", "name": "python2-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "nbformat": 4}